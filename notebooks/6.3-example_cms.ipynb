{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example for longitudinal multiple child data\n",
    "##### *(Note: in this example database tables are created in names different than the data csv filenames but identical to json metadata filenames)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --upgrade pip\n",
    "#! pip install fuzzy_sql-1.1.1b0-py3-none-any.whl\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_paths import *\n",
    "import sys\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from fuzzy_sql.fuzzy_sql import *\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_NAME='cms'\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING AND IMPORTING DATA INTO DATABASE\n",
    "##### (This is typically done for one time only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directories\n",
    "metadata_dir = os.path.join(DATA_DIR, DATASET_NAME,'processed/metadata')\n",
    "real_dir = os.path.join(DATA_DIR,DATASET_NAME, 'raw/sample1')\n",
    "syn_dir = os.path.join(DATA_DIR,DATASET_NAME, 'raw/sample2')\n",
    "db_path = os.path.join(DB_DIR, f'{DATASET_NAME}.db')\n",
    "\n",
    "\n",
    "\n",
    "# identify input data file names\n",
    "real_csvs=[\"DE1_0_2008_Beneficiary_Summary_File_Sample_1.csv\",\\\n",
    "    \"DE1_0_2009_Beneficiary_Summary_File_Sample_1.csv\",\\\n",
    "    \"DE1_0_2010_Beneficiary_Summary_File_Sample_1.csv\",\\\n",
    "    \"DE1_0_2008_to_2010_Carrier_Claims_Sample_1A.csv\",\\\n",
    "    \"DE1_0_2008_to_2010_Carrier_Claims_Sample_1B.csv\" ,\\\n",
    "    \"DE1_0_2008_to_2010_Inpatient_Claims_Sample_1.csv\",\\\n",
    "    \"DE1_0_2008_to_2010_Outpatient_Claims_Sample_1.csv\",\\\n",
    "    \"DE1_0_2008_to_2010_Prescription_Drug_Events_Sample_1.csv\" ]\n",
    "\n",
    "meta_jsons=['s1_ben_sum_2008.json','s1_ben_sum_2009.json','s1_ben_sum_2010.json','s1_carrier_1a.json',\\\n",
    "    's1_carrier_1b.json','s1_inpatient.json','s1_outpatient.json','s1_prescrp.json']\n",
    "\n",
    "syn_csvs=[\"DE1_0_2008_Beneficiary_Summary_File_Sample_2.csv\",\\\n",
    "    \"DE1_0_2009_Beneficiary_Summary_File_Sample_2.csv\",\\\n",
    "    \"DE1_0_2010_Beneficiary_Summary_File_Sample_2.csv\",\\\n",
    "    \"DE1_0_2008_to_2010_Carrier_Claims_Sample_2A.csv\",\\\n",
    "    \"DE1_0_2008_to_2010_Carrier_Claims_Sample_2B.csv\" ,\\\n",
    "    \"DE1_0_2008_to_2010_Inpatient_Claims_Sample_2.csv\",\\\n",
    "    \"DE1_0_2008_to_2010_Outpatient_Claims_Sample_2.csv\",\\\n",
    "    \"DE1_0_2008_to_2010_Prescription_Drug_Events_Sample_2.csv\" ]\n",
    "\n",
    "# Number of rows to be extracted from corresponding tables. This is only added for \"example\" purposes to reduce query times..\n",
    "rows=[10000,10000,10000,20000,20000,15000,15000,15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table s1_ben_sum_2008 already exists in the database\n",
      "Table s2_ben_sum_2008 already exists in the database\n",
      "Table s1_ben_sum_2009 already exists in the database\n",
      "Table s2_ben_sum_2009 already exists in the database\n",
      "Table s1_ben_sum_2010 already exists in the database\n",
      "Table s2_ben_sum_2010 already exists in the database\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect(db_path) #conneting to databse\n",
    "with conn:  \n",
    "    for real_csv,meta_json,syn_csv in zip(real_csvs,meta_jsons,syn_csvs):\n",
    "        #Import real\n",
    "        data,_=prep_data_for_db(os.path.join(real_dir,real_csv)) # Note: This function  can be used to quickly generate metadata template, but this is not shown here.\n",
    "        with open(os.path.join(metadata_dir, meta_json), 'r') as f:\n",
    "            metadata=json.load(f)\n",
    "        # candidate_db_idx=get_vars_to_index(metadata,data, index_vars_types='cat',cardinality_cutoff=20) # Comment out if you like to add indexing (takes a long time!!)\n",
    "        table_name=Path(meta_json).stem #real tables names are identical to json file names (without the extension)\n",
    "        make_table(table_name, data, conn) # add the argument indx_vars=candidate_db_idx if you like to index the real tables. \n",
    "        #Import syn\n",
    "        data,_=prep_data_for_db(os.path.join(syn_dir,syn_csv)) # Note: This function  can be used to quickly generate metadata template, but this is not shown here.\n",
    "        table_name=table_name.replace(\"s1\",\"s2\") #synthetic tables are the same as the real table names but with s1 replaced by s2\n",
    "        make_table(table_name, data, conn) # add the argument indx_vars=candidate_db_idx if you like to index the real tables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATING RANDOM QUERIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists with table names. Table names shall be identical to the names initially created in the database.\n",
    "real_tbl_lst=[Path(meta_json).stem for meta_json in meta_jsons]\n",
    "syn_tbl_lst=[real_tbl.replace(\"s1\",\"s2\") for real_tbl in real_tbl_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata from the provided json files into a list of dictionaries. \n",
    "# Note 1: Both real and synthetic data should have the same metadata file.\n",
    "# Note 2: Each input table in real_tbl_lst above shall have its own metadata file.\n",
    "# Note 2: The json file name shall match that of the real data file name in real_tbl_lst. \n",
    "metadata_lst = []\n",
    "for tbl_name in real_tbl_lst:\n",
    "    with open(os.path.join(metadata_dir, tbl_name+'.json'), 'r') as f:\n",
    "        metadata_lst.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_queries=gen_aggfltr_queries(3,db_path, real_tbl_lst, metadata_lst,  syn_tbl_lst )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPORTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprtr=QryRprt(real_tbl_lst, rnd_queries)\n",
    "rprtr.print_html_mltpl(f'{DATASET_NAME}.html')\n",
    "rprtr.plot_violin('Hellinger',f'{DATASET_NAME}_hlngr.png' )\n",
    "rprtr.plot_violin('Euclidean',f'{DATASET_NAME}_ecldn.png' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6cf594385e3e378fbba23be52d8fa8a1ff0f44816650af8bcee05fc5c8211531"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
